<table class="projects">
  <tbody>
    <!-- ! Deep RL for MPC Control of Quadruped Locomotion -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Hierarchical RL for MPC of Quadruped Locomotion</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/rl-mpc-locomotion?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/rl-mpc-locomotion" target="_blank">
            Code
          </a>]
          [<a href="https://docs.google.com/presentation/d/18bznpYrkCPnhCisySPDz18hvL3Ytere7JiJEbdLvpgU/edit?usp=sharing" target="_blank">
            Slides
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            Parallel Training
          </a>]
          <br>
          Fast simulation and RL training framework for a quadruped locomotion task by dynamically predicting the weight parameters of a MPC controller. The control framework is a hierarchical controller composed of a higher-level policy network and a lower-level model predictive controller.
        </p>
      </div>
    </td></tr>
    <!-- ! Motion Planning Practice -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Motion Planning Practice</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/motion-planning-practice?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/motion-planning-practice" target="_blank">
            Code
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            PDF
          </a>]
          <br>
          Fast motion planning algorithm implementations with planar hovercraft and 7-DOF KUKA arm demos in pybullet. Algorithms including Kinodynamic RRT-Connect, RRT-Connect, A*, Iterative IK, lazy-rebuilt KD-Tree and more. Code optimized for readability and efficiency.
        </p>
      </div>
    </td></tr>
    <!-- ! Bayesian Optimization for MPPI Planar Pushing -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Bayesian Optimization for MPPI Planar Pushing</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/bayesian-opt-gpytorch?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/bayesian-opt-gpytorch" target="_blank">
            Code
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            PDF
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            BOA
          </a>]
          <br>
          Self implemented Bayesian Optimization Algorithm (BOA) in PyTorch for automatically tuning the hyperparameter of Model Predictive Path Integral (MPPI) control to solve a planar box pushing task with non-trivial obstacles.
        </p>
      </div>
    </td></tr>
    <!-- ! Dynamic Object Removing SLAM with MonoRec -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Dynamic Object Removing SLAM with MonoRec</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/monorec-slam?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/monorec-slam" target="_blank">
            Code
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            PDF
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            System Diagram
          </a>]
          <br>
          We present a dynamic object removing SLAM method named MonoRec-SLAM. 
          Our method adapts the MaskModule from MonoRec and achieves an average APE improvement by 6.03% on KITTI dataset, obtains a more static map of the scenes, and achieves a great balance between real-time capability and dynamic object masking.
        </p>
      </div>
    </td></tr>
    <!-- ! Single-Image to Camera Pose with iNeRF and PoseCNN -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" width="250" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Single-Image to Camera Pose with iNeRF and PoseCNN</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/fast-inerf?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/fast-inerf" target="_blank">
            Code
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            PDF
          </a>]
          [<a href="assets/img/placeholder.jpg" target="_blank">
            System Diagram
          </a>]
          <br>
          We present an efficient and robust system for view synthesis and pose estimation by integrating PoseCNN and iNeRF. Our method leverages the pose and object segmentation predictions from PoseCNN to improve the initial camera pose estimation and accelerate the optimization process in iNeRF.
        </p>
      </div>
    </td></tr>
    <!-- ! ROB 550 Botlab -->
    <tr><td>
      <div class="project_cell">
        <video src="assets/videos/placeholder.jpg" width="250" controls muted autoplay loop></video>
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Autonomous SLAM and Exploration with MBot</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/silvery-botlab-f22?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/silvery-botlab-f22" target="_blank">
            Code
          </a>]
          [<a href="https://github.com/silvery107/silvery-botlab-f22/blob/main/data/S2T1_Botlab_Report.pdf" target="_blank">
            PDF
          </a>]
          <br>
          In the Botlab, movement control, obstacle detection, maze exploration, and self-localization functionality was developed on the MBot robot, a mobile robot platform.
          It is designed to explore the fundamentals of robot autonomy by developing MBot with autonomous mapping, localization, and exploration capabilities. 
          <b>Winner</b> among 24 teams in the final competition.
        </p>
      </div>
    </td></tr>
    <!-- ! ROB 550 Armlab -->
    <tr><td>
      <div class="project_cell">
        <img src="assets/img/placeholder.jpg" height="190" class="zoom">
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Robust Detecting and Palletizing with Robot Arm</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/silvery-armlab-f22?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/silvery-armlab-f22" target="_blank">
            Code
          </a>]
          [<a href="https://github.com/silvery107/silvery-armlab-f22/blob/master/config/s2_t5_armlab_f22_CV.pdf" target="_blank">
            CV PDF
          </a>]
          [<a href="https://github.com/silvery107/silvery-armlab-f22/blob/master/config/s2_t5_armlab_f22_RC.pdf" target="_blank">
            Control PDF
          </a>]
          <br>
          In the Armlab, a 5-DOF robotic arm fully autonomously arranges blocks of different sizes, colors and positions into the desired arrangement. Analytical inverse kinematics is used to determine the appropriate waypoints. An overhead LiDAR Camera is utilized to identify blocks on the board.
          <b>Winner</b> among 24 teams in the final competition.
        </p>
      </div>
    </td></tr>
    <!-- ! Agile Waste Sorting with Tossing -->
    <tr><td>
      <div class="project_cell">
        <video src="assets/videos/placeholder.jpg" width="250" controls muted autoplay loop></video>
      </div>
      <div class="project_cell">
        <p style="font-size:18px;margin-bottom: 0px;">
          <font color="black"><b>Agile Waste Sorting with Tossing</b></font>
        </p>
        <p>
          <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/silvery107/ME336-Yellow-Team-Project?style=plastic&logo=github">
          [<a href="https://github.com/silvery107/ME336-Yellow-Team-Project" target="_blank">
            Code
          </a>]
          [<a href="https://github.com/silvery107/ME336-Yellow-Team-Project/blob/main/projects/ME336_Report.pdf" target="_blank">
            PDF
          </a>]
          <br>
          Implemented the automatic collection and cleaning of images based on MOG2 algorithm.
          Deployed and trained YOLOv5 to achieve quick waste classification.
          Achieved planning for robot arm to pick toss waste on dynamic conveyor belt.
        </p>
      </div>
    </td></tr>
  </tbody>
</table>
